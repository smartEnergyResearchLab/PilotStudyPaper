---
title: 'Response model testing'
subtitle: 'Exploring D-W stat and platting methods'
author: 'Ben Anderson'
date: 'Last run at: `r Sys.time()`'
output:
  bookdown::html_document2:
    fig_caption: yes
    code_folding: hide
    number_sections: yes
    toc: yes
    toc_depth: 2
    toc_float: TRUE
    self_contained: TRUE
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
---

# Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table) # big fast data
library(skimr) # summary on steroids
library(ggplot2) # pretty plots
library(kableExtra) # pretty tables
library(broom) # tidy up regression outputs
library(car) # model diagnostics
library(here) # so useful - relative repo paths!
library(stargazer) # pretty model tables

user <- Sys.info()[[7]]
dataF <- path.expand("~/temp/serlRecruit18000.csv")
```

# Introduction

Testing non-response models for the SERL pilot data

# Data

18,000 cases - the households to whom letters were sent

```{r loadData}
all_onboards <- data.table::fread(dataF)

skimr::skim(all_onboards)
```

# Descriptive stats

We care about:

 * `consent` - if they gave consent for smart meter data to be collected
 * `region` - govt office region of address (England & Wales)
 * `consent_source` - postal or online
 * `incentive` - the incentive they received (if any)
 * `p2w` - whether they were push to web or not
 * `quintile` - IMD quintile of the LSOA the address is in, 1 = most deprived (XX check coding XX)
 * `cell` - the combination of recruitment processes received
 * `version` - ?

First we'll look at consent rates by region. We wouldn't really expect a regional consent effect...

```{r consentByRegion, fig.cap="% consent by region"}
tr <- table(all_onboards$region, all_onboards$consent)
kableExtra::kable(addmargins(tr), caption = "Number of consents by region", digits = 2) %>%
  kable_styling()
trp <- prop.table(tr, 1)*100
kableExtra::kable(trp, caption = "% consents by region", digits = 2) %>%
  kable_styling()

plotDT <- as.data.table(trp)
ggplot2::ggplot(plotDT[V2 == "TRUE"], aes(y = V1, x = N)) +
  geom_col() +
  labs(x = "% consent",
       y = "Region")

# how do we order x axis by size of y value?
```
There are some slight differences between regions but nothing too notable. It may be the case that regional effects are confounded by IMD (etc) effects.

Repeat this for IMD - plots need CI

```{r consentByIMD, fig.cap="% consent by IMD quintile"}
tr <- table(all_onboards$quintile, all_onboards$consent)
kableExtra::kable(addmargins(tr), caption = "Consents by IMD quintile", digits = 2) %>%
  kable_styling()
trp <- prop.table(tr, 1)*100
kableExtra::kable(trp, caption = "% consents by IMD quintile", digits = 2) %>%
  kable_styling()

plotDT <- as.data.table(trp)
ggplot2::ggplot(plotDT[V2 == "TRUE"], aes(y = V1, x = N)) +
  geom_col() +
  labs(x = "% consent",
       y = "IMD quintile")
```
In this case we do see more of a relationship - consent rates also increase as IMD quintile increases (less deprived).

Do we have relatively evenly distributed cases by IMD and region? If we don't then a correlation between IMD and consent rates may lead to an apparent (spurious) regional non-consent rate.

```{r crossTabPlot, fig.cap="Counts of addresses contacted by region and IMD quintile"}
t <- table(all_onboards$region, all_onboards$quintile)
t # table of case frequency by IMD & region
round(prop.table(t,1)*100,2)
# interesting dominance of IMD Q5 in South East

dt <- as.data.table(t)

ggplot2::ggplot(dt, aes(y = V1, x = V2, fill = N)) +
  geom_tile() +
  labs(y = "Region",
       x = "IMD Quintile")
```

Figure \@ref(fig:crossTabPlot) (and the preceeding table) show that we have some IMD quintiles in some regions that have more cases than we might expect. This is likely to reflect:

 * a preponderance of those IMD quintiles in that region and/or
 * an over-sampling of those IMD quintiles due to them being more likely to have smart meters already installed.

The latter seems unlikely...

We can repeat this analysis using `gmodels::CrossTable` which gives both counts and proportions.

```{r, crossTables}
# table of row % with last row as frequency by IMD quin
# requires gmodels
gmodels::CrossTable(all_onboards$region, all_onboards$quintile, 
           prop.chisq = FALSE,
           prop.r = TRUE, # row proportions
           prop.t = FALSE, # no table props
           prop.c = FALSE, # col proportions
           chisq = TRUE,
           dnn = c("Outcome","IMD quintile") )#how do we turn off observed frequencies?
           
```

So both Figure \@ref(fig:crossTabPlot) and the CrossTab results are telling us that:

 * IMD 5 appears over-represented in South East (but perhaps most SE LSOAs are in IMD 5?)
 * IMD 2 appears over-represented in London
 * IMD 1 appears over-represented in West Midlands

> But: we need to check if these row % distributions match the IMD * Region distribution of LSOAs? If they do then there is no problem. If not then have we got over/under sampling going on by IMD?

# Simple non-response model

## Orignal data as loaded

Next we'll build a simple non-response model and test it.

 * outcome/dependent variable : consent
 * independent variables: region + quintile

```{r modelOrig}
table(all_onboards$consent, useNA = "always") # check for NA
prop.table(table(all_onboards$consent, useNA = "always")) # check for NA
m <- glm(formula = consent ~ as.factor(quintile) + 
                          region, 
                       family = binomial(logit), all_onboards) # wiil auto-drop the missing values

summary(m)
```

As we might expect from the descriptive tables, the results suggest region is a relatively weak predictor of consent but consent rates increase in higher (least deprived) IMD areas.

Run diagnostics to check the model...

```{r modelOrigDW}

# Independence of errors
car::durbinWatsonTest(m)
# if p < 0.05 then a problem as implies autocorrelation

```

DW is very close to 0 which implies strong autocorrelation of the errors. This is odd. Let's check the residuals to see what is going on.

```{r, modelOrigDW_residsPlot}
plot(m$residuals)
```

So that looks like the first n rows are 'TRUE' and the last n rows are 'FALSE'. This will cause the DW stat to go AWOL...

We'll run the other diagnostics just to check. First collinearity.

```{r, modelOrigCol}
# requires library(car)
# Collinearity (vif)
car::vif(m)
# if any values > 10 -> problem

# Collinearity (tolerance)
1/car::vif(m)
# if any values < 0.2 -> possible problem 
# if any values < 0.1 -> definitely a problem=
```

Looks OK.

Now model diagnostic plots.

```{r modelOrigDiagPlots, fig.cap="Diagnostic plots: original"}
# Diagnostic plots ----
plot(m)

car::spreadLevelPlot(m)
# Suggested power transformation:  0.4744183 
```

Also looks OK.

```{r, saveOrig}
# CIs for coef not OR for plot
coefCI <- cbind(coef = coef(m), 
                confint(m))

df <- broom::tidy(m)
df$model <- "1. Original"

originalModelDT <- data.table::as.data.table(cbind(coefCI, df)) # collect for later plot

mOrig <- m
```

## Randomly shuffled data

If we are correct then we need to randomly shuffle the data unless we care about it's order - for example if we though there might be a relationship between consent rates and dates.

```{r modelShuffle}
# OK, so let's shuffle the original data randomly
all_onboards <- all_onboards[sample(nrow(all_onboards)),]

m <- glm(formula = consent ~ as.factor(quintile) + 
                          region, 
                       family = binomial(logit), all_onboards) # wiil auto-drop the missing values

summary(m)
```

The model results should be identical - we'll double check this by plotting them sude by side later.

Now, check the D-W...

```{r modelShuffledDW}

# Independence of errors
car::durbinWatsonTest(m)
# if p < 0.05 then a problem as implies autocorrelation

```

Good, that's more what we expected. D-W is close to 2 which indicates no auto-correlation of errors. The p value is driven by the n and is of less diagnostic value.

Check the residuals plot.

```{r, modelShuffledDW_residsPlot}
plot(m$residuals)
```

Yep, random noise

We'll run the other diagnostics just to check. First collinearity.

```{r, modelShuffledCol}
# requires library(car)
# Collinearity (vif)
car::vif(m)
# if any values > 10 -> problem

# Collinearity (tolerance)
1/car::vif(m)
# if any values < 0.2 -> possible problem 
# if any values < 0.1 -> definitely a problem=
```

Looks OK.

Now model diagnostic plots.

```{r modelShuffledDiagPlots, fig.cap="Diagnostic plots: original"}
# Diagnostic plots ----
plot(m)

car::spreadLevelPlot(m)
# Suggested power transformation:  0.4744183 
```

Also looks OK.

```{r, saveShuffled}
# CIs for coef not OR for plot
coefCI <- cbind(coef = coef(m), 
                confint(m))

df <- broom::tidy(m)
df$model <- "2. Shuffled"

shuffledModelDT <- data.table::as.data.table(cbind(coefCI, df)) # collect for later plot
mShuffled <- m
```

## Compare the models

We can plot the two model coefficients and CIs side by side to check they are the same. This is also a useful way to visualise the model results.

```{r, compareSimpleModels, fig.cap="Comparing simple consent models"}
plotDT <- rbind(originalModelDT, shuffledModelDT)
data.table::setnames(plotDT, c("2.5 %","97.5 %"), c("lo", "up")) # for easier ggplotting

p <- ggplot2::ggplot(plotDT, aes(x = coef, y = term , fill = model)) + 
  geom_col(position = "dodge") +
  geom_errorbar(aes(xmax = up, xmin = lo),position = "dodge") +
  geom_vline(xintercept = 0) +
  theme(legend.position="bottom")
p

ggplot2::ggsave("compareOriginalShuffled.png", plot = p,
                path = here::here("plots"))

```

As we can see the model results are identical. We can also see that the intercept is doing a  lot of work for us, we have a couple of region effects and we can be more confident about the IMD effects as we have controlled for region...

At this stage we'll generate a pretty model output table for the shuffled model (although it doesn't matter which we choose). For this we use the `stargazer` package.  And it saves this table for us too.

```{r, simpleModelOutput,results='asis'}
stargazer::stargazer(mShuffled,
                     type = "html", 
                     title="Shuffled Model", digits=3, 
                     out=here::here("outputTables","shuffledModel.txt"))
```


# Complex non-response model: adding model co-variates

Having established our base model we'll now add `cell` to see which pathway made a difference.

```{r, complexModel}
m <- glm(formula = consent ~ as.factor(quintile) + 
                          region + 
           as.factor(cell), 
                       family = binomial(logit), all_onboards) # wiil auto-drop the missing values

summary(m)
# CIs for coef not OR for plot
coefCI <- cbind(coef = coef(m), 
                confint(m))

df <- broom::tidy(m)
df$model <- "3. Complex"

complexModelDT <- data.table::as.data.table(cbind(coefCI, df)) # collect for later plot

mComplex <- m
```

There are a number of cell pathways which have effects of some size (2,4,8,10) but only cell 8 is statistically significant at the 95% level. Cell 2 has a notable large +ve effect while cell 4 has a notably large -ve effect but the SE is large in both cases so the 95% CI are also likely to be wide It is possible that this uncertainty is likely to be producing the n/s result.

# Comparing all our models

We can see how much this modifies the original simple model by plotting them together. This will be quite a busy plot...

```{r, compareAllModels, fig.cap="Comparing consent models"}
plotDT <- rbind(shuffledModelDT, complexModelDT)
data.table::setnames(plotDT, c("2.5 %","97.5 %"), c("lo", "up")) # for easier ggplotting

p <- ggplot2::ggplot(plotDT, aes(x = coef, y = term , fill = model)) + 
  geom_col(position = "dodge") +
  geom_errorbar(aes(xmax = up, xmin = lo),
                position = "dodge") +
  geom_vline(xintercept = 0) +
  theme(legend.position="bottom")
p

ggplot2::ggsave("compareAllModels.png", plot = p,
                path = here::here("plots"))

```
The plot confirms the previous conclusion that while there are some potentially interesting effect sizes, only cell 8 is statistically significant at the 95% level as indicated by the 95% CI not including 0. Hoever we should not ignore cell 2 which is clearly marginal.

It also doesn't look like adding the `cell` variable alters the other parts of the model much (the other coefficients appear largely unchanged) but it might be better to have them side by side in a table. Stargazer can do that too... and it saves this table for us as well

```{r, allModelOutput, results='asis'}
stargazer::stargazer(mShuffled, mComplex,
                     type = "html", 
                     title="Shuffled Model", digits=3, 
                     out=here::here("outputTables","allModels.txt"))
```

Finally we can test if the model with `cell` is an improvement using `anova` with the Chisq test method (as our models are logistic):

```{r, testModels}
anova(mShuffled, mComplex, test = "Chisq")

```

Looks like it does.

# R environment

```{r, envLog}
sessionInfo()
```
